# Phase 5: Audit, Monitoring & Compliance Operations Guide

This guide covers the operational aspects of AI-Box Phase 5: configuring the audit event pipeline, connecting to your SIEM, managing immutable log storage, and verifying log integrity.

---

## 1. Audit Event Schema

All AI-Box components emit structured JSON events in a common schema. Each event contains:

| Field | Type | Description |
|-------|------|-------------|
| `timestamp` | string (RFC 3339) | When the event occurred |
| `event_type` | string | Event category (see table below) |
| `sandbox_id` | string | Container/sandbox identifier |
| `user_id` | string | Developer who owns the sandbox |
| `source` | string | Component that generated the event |
| `severity` | string | `info`, `warning`, `high`, or `critical` |
| `details` | object | Event-specific key-value pairs |
| `hash_prev` | string | SHA-256 of the previous event (hash chain) |

### 1.1 Event Types

| Category | Event Types | Retention |
|----------|-------------|-----------|
| Sandbox lifecycle | `sandbox.create`, `sandbox.start`, `sandbox.stop`, `sandbox.destroy`, `sandbox.config` | 2+ years |
| Network | `network.allow`, `network.deny` | 1+ year |
| DNS | `dns.query`, `dns.response` | 1+ year |
| Tool invocations | `tool.invoke`, `tool.approve`, `tool.deny` | 2+ years |
| Credentials | `credential.issue`, `credential.use`, `credential.rotate`, `credential.revoke` | 2+ years |
| Policy decisions | `policy.allow`, `policy.deny` | 2+ years |
| LLM API traffic | `llm.request`, `llm.response` | 1+ year |
| File access | `file.read`, `file.write` | 1+ year |
| Falco alerts | `falco.alert` | 1+ year |
| Session recording | `session.start`, `session.end` | 2+ years |

### 1.2 Sources

| Source ID | Component |
|-----------|-----------|
| `aibox-cli` | CLI (`aibox` binary) |
| `aibox-agent` | In-container agent |
| `aibox-llm-proxy` | LLM API sidecar proxy |
| `squid` | Squid HTTP/HTTPS proxy |
| `coredns` | CoreDNS resolver |
| `opa` | Open Policy Agent |
| `vault` | HashiCorp Vault |
| `falco` | Falco runtime monitor |
| `auditd` | Host audit daemon |
| `session-recorder` | Terminal session recorder |

### 1.3 Hash Chain

Every event includes a `hash_prev` field containing the SHA-256 hash of the previous event's serialized JSON. This creates a tamper-evident chain:

```
Event 0:  hash_prev = 0000...0000 (genesis)
Event 1:  hash_prev = SHA-256(Event 0 JSON)
Event 2:  hash_prev = SHA-256(Event 1 JSON)
...
```

If any event is modified, deleted, or reordered, the chain breaks at the next event. Use `aibox audit verify` to check chain integrity (see Section 4).

### 1.4 Example Event

```json
{
  "timestamp": "2026-02-21T10:30:00.000000000Z",
  "event_type": "network.deny",
  "sandbox_id": "aibox-dev1-abc123",
  "user_id": "dev1",
  "source": "squid",
  "severity": "warning",
  "details": {
    "destination": "evil.com",
    "port": 443,
    "method": "CONNECT",
    "bytes": 0
  },
  "hash_prev": "a3f2b8c1d4e5f6..."
}
```

---

## 2. Vector Pipeline Configuration

Vector is the log collection agent that runs on each developer workstation. It collects events from all AI-Box components, normalizes them, and ships them to the configured sink.

### 2.1 Configuration File

The Vector config is generated by `aibox setup` and written to `/etc/aibox/vector.toml`. To regenerate:

```bash
# Regenerate with defaults
aibox setup --component vector

# Or specify a custom config path
aibox setup --component vector --vector-config /custom/path/vector.toml
```

### 2.2 Sources

Vector collects from 4-5 source types depending on the `runtime_backend` setting:

| Source | Type | Path/Config | Events Collected |
|--------|------|-------------|-----------------|
| `aibox_audit` | file | `/var/log/aibox/audit.jsonl` | All structured audit events |
| `squid_proxy` | file | `/var/log/aibox/proxy-access.log` | Network connections (allowed + denied) |
| `opa_decisions` | file | `/var/log/aibox/decisions.jsonl` | Policy evaluation decisions |
| `falco_alerts` | file | `/var/log/aibox/falco-alerts.jsonl` | Falco runtime alerts (when `runtime_backend = "falco"`) |
| `auditd_events` | journald | Unit: auditd | Kernel audit events (when `runtime_backend = "auditd"`) |
| `journald_aibox` | journald | Units: aibox-agent, aibox-llm-proxy, coredns-aibox, squid[, falco] | Systemd service logs |

### 2.3 Transforms

Each source has a corresponding transform that:
1. Parses structured data (JSON or log format)
2. Enriches with `hostname`, `cluster`, and `classification_level` context
3. Adds `pipeline_ts` (Vector processing timestamp)

| Transform | Input | Purpose |
|-----------|-------|---------|
| `parse_audit` | aibox_audit | Parse JSON, add host/cluster/classification context |
| `parse_squid` | squid_proxy | Tag with source=squid, add context |
| `parse_decisions` | opa_decisions | Parse JSON, tag source=opa |
| `parse_runtime` | falco_alerts or auditd_events | Parse runtime security events (conditional on `runtime_backend`) |
| `filter_llm` | parse_audit | Filter LLM events and apply `llm_logging_mode` policy |
| `enrich_journald` | journald_aibox | Add host/cluster to journald events |

### 2.4 Policy-Driven Configuration

Three config fields control Vector pipeline behavior:

#### `runtime_backend` (default: `"falco"`)

Controls which runtime security source is configured:

| Value | Source Created | Transform Behavior |
|-------|---------------|-------------------|
| `falco` | `[sources.falco_alerts]` (file) | Parses JSON, sets `source = "falco"`, `event_type = "falco.alert"` |
| `auditd` | `[sources.auditd_events]` (journald) | Sets `source = "auditd"`, `event_type = "auditd.event"` |
| `none` | No runtime source | No `parse_runtime` transform; excluded from sink inputs |

#### `llm_logging_mode` (default: `"full"`)

Controls how LLM request/response payloads are handled in the `filter_llm` transform:

| Mode | Behavior |
|------|----------|
| `full` | Pass through all LLM event data unmodified |
| `hash` | Replace `details.request_body` and `details.response_body` with SHA-256 hashes |
| `metadata_only` | Strip `request_body`, `response_body`, `prompt`, and `completion` fields entirely |

Use `hash` or `metadata_only` for environments where LLM payload content must not be stored (privacy regulations, classified environments).

#### `classification_level` (default: `"standard"`)

Enriches every event with a `classification_level` field. Set to `"classified"` for environments subject to stricter audit requirements. This field is available in SIEM queries for filtering and routing.

### 2.5 Sink Types

| Sink | Use Case | Config Key |
|------|----------|------------|
| `file` | Local development, debugging | `audit.sink_type = "file"` |
| `http` | SIEM integration (Splunk, Elastic, Sentinel) | `audit.sink_type = "http"` |
| `s3` | Immutable storage (MinIO, AWS S3) | `audit.sink_type = "s3"` |
| `console` | Debugging (stdout) | `audit.sink_type = "console"` |

### 2.6 Buffering

All persistent sinks (file, HTTP, S3) use disk-backed buffering for at-least-once delivery:

- **Default buffer size**: 500 MB
- **Buffer location**: `/var/lib/aibox/vector/`
- **When full**: Blocks (backpressure) to prevent event loss
- **Override**: Set `AIBOX_AUDIT_BUFFER_MAX_BYTES` or edit `vector.toml`

### 2.7 Configuration Reference

Key settings in `~/.config/aibox/config.yaml`:

```yaml
audit:
  enabled: true
  log_path: /var/log/aibox/audit.jsonl
  vector_config_path: /etc/aibox/vector.toml
  storage_backend: local          # local, minio, or s3
  storage_endpoint: ""            # required for minio/s3
  storage_bucket: aibox-audit
  retention_tier1: "730d"         # lifecycle/policy/credential/tool events
  retention_tier2: "365d"         # network/DNS/LLM/file events
  runtime_backend: falco          # falco, auditd, or none
  llm_logging_mode: full          # full, hash, or metadata_only
  classification_level: standard  # standard or classified
```

### 2.8 Health Checks

Vector exposes an API on `127.0.0.1:8686` for health monitoring:

```bash
# Check if Vector is running
aibox doctor --check vector

# Manual health check
curl -s http://127.0.0.1:8686/health | jq .
```

### 2.9 Troubleshooting

| Symptom | Cause | Fix |
|---------|-------|-----|
| Events not arriving at SIEM | Vector not running | `systemctl status vector`, check `aibox doctor` |
| High disk usage in `/var/lib/aibox/vector/` | Sink unreachable, buffer filling | Check sink endpoint connectivity, review Vector logs |
| Duplicate events | Vector restarted mid-batch | Normal with at-least-once delivery; SIEM should deduplicate by event hash |
| Missing source in events | Component not logging to expected path | Verify log paths match Vector source config |
| Vector consuming too much CPU | Too many file sources or high event volume | Check Vector metrics at `:8686/api/v1/metrics` |

---

## 3. SIEM Integration Guide

AI-Box integrates with any SIEM platform that accepts structured JSON logs. Vector's sink flexibility supports all major SIEMs.

### 3.1 Supported Transport Methods

| Method | SIEM Platforms | Vector Sink |
|--------|---------------|-------------|
| HTTP/JSON | Splunk HEC, Elastic, Microsoft Sentinel, Datadog | `http` |
| Syslog (TCP) | QRadar, ArcSight, any syslog receiver | `syslog` (socket) |
| S3/MinIO | Splunk SmartStore, Elastic S3 input, custom | `s3` (aws_s3) |
| Kafka | Any SIEM with Kafka consumer | `kafka` |

### 3.2 Quick Start: Connect to Your SIEM

**Step 1**: Identify your SIEM's ingestion endpoint and method.

**Step 2**: Configure AI-Box:

```yaml
# ~/.config/aibox/config.yaml
audit:
  enabled: true
  # Choose one:
  # sink_type: http
  # sink_endpoint: "https://siem.internal/api/v1/ingest"
```

**Step 3**: Regenerate Vector config:

```bash
aibox setup --component vector
```

**Step 4**: Verify events arrive:

```bash
# Generate a test event
aibox start --workspace /tmp/test-workspace
aibox stop

# Check SIEM for events with event_type=sandbox.create
```

### 3.3 Platform-Specific Configuration

#### Splunk (HTTP Event Collector)

```yaml
audit:
  sink_type: http
  sink_endpoint: "https://splunk.internal:8088/services/collector/event"
```

Set the HEC token via environment variable or Vector config override. Events arrive in Splunk as `sourcetype=aibox:audit`.

#### Elastic SIEM / OpenSearch

```yaml
audit:
  sink_type: http
  sink_endpoint: "https://elastic.internal:9200/aibox-audit/_bulk"
```

Create an index template for `aibox-audit-*` with mappings matching the event schema.

#### Microsoft Sentinel

```yaml
audit:
  sink_type: http
  sink_endpoint: "https://<workspace-id>.ods.opinsights.azure.com/api/logs?api-version=2016-04-01"
```

Use the Log Analytics Data Collector API. Set `Log-Type: AIBoxAudit` header.

#### QRadar (Syslog)

```yaml
audit:
  sink_type: syslog
  sink_endpoint: "qradar.internal:514"
```

Create a custom log source in QRadar for AI-Box events.

#### Kafka-Based Pipeline

```yaml
audit:
  sink_type: kafka
  sink_endpoint: "kafka.internal:9092"
```

Events are published to the `aibox-audit-events` topic. Configure your SIEM's Kafka consumer to read from this topic.

### 3.4 Detection Rules

AI-Box ships 10 pre-built detection rules. Import these into your SIEM as correlation rules or saved searches.

| ID | Rule Name | Severity | Trigger | Sources |
|----|-----------|----------|---------|---------|
| aibox-001 | Anomalous Outbound Data Volume | High | >100 MB transferred in 30 min from one sandbox | Squid |
| aibox-002 | DNS Query Spike | Medium | >100 queries/min from one sandbox | CoreDNS |
| aibox-003 | Off-Hours Credential Access | Medium | Credential issued outside business hours | Vault |
| aibox-004 | Repeated Blocked Network Attempts | High | >20 denied requests in 10 min from one sandbox | Squid, nftables |
| aibox-005 | LLM Payload Size Anomaly | Medium | Request payload >1 MB (3x 95th percentile) | LLM proxy |
| aibox-006 | Container Escape Indicator | Critical | Falco critical alert fired | Falco |
| aibox-007 | Policy Violation Burst | High | >15 policy denials in 5 min from one sandbox | OPA |
| aibox-008 | Credential Access After Sandbox Stop | Critical | Credential use after sandbox.stop event | Vault, CLI |
| aibox-009 | Base64 Payload Anomaly | Medium | >10 KB base64 blocks in LLM request fields | LLM proxy |
| aibox-010 | Git Push to Unexpected Remote | High | Git push to remote not in allowlist | Agent |

### 3.5 Alert Routing

| Severity | Channel | Response SLA | Example Rules |
|----------|---------|-------------|---------------|
| Critical | PagerDuty (on-call page) | 15 minutes | aibox-006, aibox-008 |
| High | Security team Slack channel | 4 hours | aibox-001, aibox-004, aibox-007, aibox-010 |
| Medium | Security team email digest | Next business day | aibox-002, aibox-003, aibox-005, aibox-009 |
| Info | Dashboard only | Weekly review | (custom rules) |

### 3.6 MITRE ATT&CK Mapping

| Rule | MITRE Technique |
|------|----------------|
| aibox-001 | T1041 (Exfiltration Over C2 Channel) |
| aibox-002 | T1071.004 (Application Layer Protocol: DNS) |
| aibox-004 | T1090 (Proxy) |
| aibox-005 | T1567 (Exfiltration Over Web Service) |
| aibox-006 | T1611 (Escape to Host) |
| aibox-008 | T1528 (Steal Application Access Token) |
| aibox-009 | T1132.001 (Data Encoding: Standard Encoding) |
| aibox-010 | T1567.001 (Exfiltration to Code Repository) |

---

## 4. Immutable Log Storage

Audit logs are stored in a tamper-evident, append-only format. The storage layer provides two levels of protection:

1. **Application layer**: Cryptographic hash chain detects event modification or deletion
2. **Storage layer**: Backend-specific immutability (Object Lock, read-only files)

### 4.1 Storage Backends

| Backend | Best For | Immutability Mechanism |
|---------|----------|----------------------|
| `local` | Development, small deployments | Read-only files (0444), hash chain |
| `minio` | On-premises, air-gapped | MinIO Object Lock (Compliance mode) |
| `s3` | Cloud-connected | S3 Object Lock (Compliance mode) |

### 4.2 Configuration

```yaml
# ~/.config/aibox/config.yaml
audit:
  storage_backend: local          # local, minio, or s3
  storage_endpoint: ""            # MinIO/S3 endpoint (required for minio/s3)
  storage_bucket: aibox-audit     # bucket name
  retention_tier1: "730d"         # 2 years for lifecycle/policy/credential/tool
  retention_tier2: "365d"         # 1 year for network/DNS/LLM/file
```

#### Local Backend

Events are stored as JSON batch files under `/var/lib/aibox/audit/`. Each batch file:
- Is written with read-only permissions (0444)
- Contains a SHA-256 checksum of all entries
- Cannot be overwritten (append-only semantics enforced by the backend)

#### MinIO Backend

```yaml
audit:
  storage_backend: minio
  storage_endpoint: "https://minio.internal:9000"
  storage_bucket: aibox-audit
```

Prerequisites:
- MinIO server with Object Lock enabled on the bucket
- Bucket created with: `mc mb --with-lock minio/aibox-audit`
- Object Lock retention set to Compliance mode

#### S3 Backend

```yaml
audit:
  storage_backend: s3
  storage_endpoint: ""   # uses default AWS endpoint
  storage_bucket: aibox-audit
```

Prerequisites:
- S3 bucket with Object Lock enabled (must be set at bucket creation)
- Bucket versioning enabled
- Default retention: Compliance mode, matching retention_tier1/tier2

### 4.3 Verifying Log Integrity

The `aibox audit verify` command walks the entire hash chain and checks batch checksums:

```bash
# Verify all stored audit logs
aibox audit verify

# Example output (intact):
# Verification complete.
#   Batches checked:  142
#   Events verified:  28,419
#   Chain integrity:  INTACT
#   Corrupt batches:  0

# Example output (tampered):
# Verification complete.
#   Batches checked:  142
#   Events verified:  12,003
#   Chain integrity:  BROKEN at event 12,004
#   Corrupt batches:  1
#   First error:      event 12004: hash chain broken (expected a3f2..., got b1c4...)
```

Run verification:
- **Daily**: Automated via cron or systemd timer
- **On demand**: After any suspected incident
- **Before compliance audits**: Generate a verification report

### 4.4 Retention Policies

| Tier | Event Categories | Minimum Retention | Config Key |
|------|-----------------|-------------------|------------|
| Tier 1 | Sandbox lifecycle, tool invocations, credentials, policy decisions | 2 years (730 days) | `audit.retention_tier1` |
| Tier 2 | Network, DNS, LLM traffic, file access, Falco alerts | 1 year (365 days) | `audit.retention_tier2` |

For MinIO/S3 backends, retention is enforced by Object Lock lifecycle rules. For the local backend, a cleanup job removes batches older than the configured retention.

### 4.5 Storage Sizing

Estimated storage requirements for 200 developers:

| Metric | Estimate |
|--------|----------|
| Events per developer per day | 500-2,000 |
| Average event size | 500 bytes |
| Daily volume (200 devs, uncompressed) | 50 MB - 200 MB |
| Daily volume (compressed, gzip) | 10 MB - 40 MB |
| 1-year storage (Tier 2, compressed) | 3.6 GB - 14.6 GB |
| 2-year storage (Tier 1, compressed) | 7.3 GB - 29.2 GB |

Session recordings (if enabled) add 500 MB - 1.5 GB/day compressed.

---

## 5. Operational Runbook

### 5.1 Daily Tasks

| Task | Command | Purpose |
|------|---------|---------|
| Verify log integrity | `aibox audit verify` | Detect tampering |
| Check Vector health | `aibox doctor --check vector` | Ensure pipeline is running |
| Review SIEM alerts | (SIEM dashboard) | Respond to security events |

### 5.2 Incident Response: Tampered Logs Detected

1. **Isolate**: Do not restart or modify the affected system.
2. **Preserve**: Copy the affected batch files for forensic analysis.
3. **Investigate**: The `first_error` in the verify output indicates the exact break point.
4. **Compare**: If MinIO/S3 backend, check Object Lock status to determine if storage-level immutability was also bypassed.
5. **Escalate**: Tampered audit logs indicate a sophisticated attack. Engage the security team immediately.

### 5.3 Incident Response: Vector Pipeline Down

1. **Check status**: `systemctl status vector` and `aibox doctor --check vector`
2. **Check buffer**: Events are buffered to disk. No events are lost if Vector restarts within the buffer capacity (default 500 MB).
3. **Restart**: `systemctl restart vector`
4. **Verify catchup**: After restart, check that buffered events are flushed to the sink.

### 5.4 Rotating Audit Logs

The local audit log file (`/var/log/aibox/audit.jsonl`) rotates automatically:
- **Max size**: 100 MB per file (configurable)
- **Rotated files**: Up to 9 rotated files (.1 through .9)
- **Hash chain**: Continues seamlessly across rotations and logger restarts

---

## 6. Incident Investigation Playbook

This section provides step-by-step procedures for investigating the most common security scenarios detected by AI-Box monitoring. Each playbook follows the same structure: alert trigger, initial triage, log queries, investigation steps, and response actions.

### 6.1 Container Escape Attempt (Falco Critical Alert)

**SIEM Rule**: aibox-006 (Container Escape Indicator)
**Severity**: Critical | **SLA**: 15 minutes | **Channel**: PagerDuty
**MITRE ATT&CK**: T1611 (Escape to Host)

**Alert trigger**: Falco fires a critical-severity rule such as `aibox_ptrace_attempt`, `aibox_raw_socket_open`, `aibox_write_outside_allowed_dirs`, `aibox_write_sensitive_auth_files`, or `aibox_proc_environ_read`.

**Step 1 -- Initial triage (0-5 min)**

Identify the affected sandbox and developer:

```
# SIEM query: Falco critical alerts in the last hour
event_type:falco.alert AND severity:critical
| sort timestamp desc
| fields timestamp, sandbox_id, user_id, details.rule, details.command, details.file
```

Confirm the alert is not a false positive. Common false positives:
- Build tools (gcc, ld) writing to `/tmp` subdirectories -- check if `details.file` is under an allowed path
- IDE language servers reading `/proc/self/environ` -- check if `details.command` is a known IDE process

**Step 2 -- Scope the incident (5-10 min)**

Query all events from the same sandbox in the alert window:

```
# All events from the affected sandbox in the past 30 minutes
sandbox_id:"<sandbox_id>" AND timestamp:[now-30m TO now]
| sort timestamp asc
```

Look for:
- Network deny events before the escape attempt (reconnaissance)
- Policy violations (denied tool invocations)
- Unusual file access patterns

**Step 3 -- Check for lateral movement**

```
# Network connections from the sandbox
sandbox_id:"<sandbox_id>" AND event_type:network.*
| fields timestamp, details.destination, details.port, details.bytes
```

```
# Credential activity from the same user
user_id:"<user_id>" AND event_type:credential.*
| sort timestamp desc
```

**Step 4 -- Response actions**

| Finding | Action |
|---------|--------|
| Confirmed escape attempt | Stop sandbox immediately: `aibox stop --sandbox <id>`. Revoke all credentials issued to the sandbox. Preserve audit logs and session recording. |
| gVisor/seccomp blocked the attempt | The defense-in-depth worked. Document the attempt. Review if the Falco rule thresholds need adjustment. |
| False positive | Tune the Falco rule. Add the legitimate process to the allowlist in `aibox-policies`. |

**Step 5 -- Post-incident**

1. Verify hash chain integrity for the affected time window: `aibox audit verify`
2. If session recording was enabled, review the recording: `aibox audit playback --session <session_id>`
3. File incident report with the sandbox ID, user, timeline, and response actions taken

---

### 6.2 Suspected Data Exfiltration via LLM API

**SIEM Rules**: aibox-005 (LLM Payload Size Anomaly), aibox-009 (Base64 Payload Anomaly)
**Severity**: Medium | **SLA**: Next business day | **Channel**: Email
**MITRE ATT&CK**: T1567 (Exfiltration Over Web Service), T1132.001 (Data Encoding)

**Alert trigger**: The LLM proxy detects a request payload significantly larger than the 95th percentile (>3x), or detects base64-encoded blocks in non-content fields.

**Step 1 -- Identify the anomalous requests**

```
# Large LLM requests from the sandbox
sandbox_id:"<sandbox_id>" AND event_type:llm.request
| where details.payload_size > 1048576
| sort details.payload_size desc
| fields timestamp, user_id, details.payload_size, details.model, details.endpoint
```

**Step 2 -- Analyze payload patterns**

```
# Base64-encoded content in LLM requests
sandbox_id:"<sandbox_id>" AND event_type:llm.request AND details.base64_detected:true
| fields timestamp, details.payload_size, details.base64_size, details.field_name
```

Compare against the developer's normal usage:

```
# Developer's LLM usage baseline (past 7 days)
user_id:"<user_id>" AND event_type:llm.request AND timestamp:[now-7d TO now]
| stats avg(details.payload_size) as avg_size, p95(details.payload_size) as p95_size, count
```

**Step 3 -- Correlate with file access**

```
# File reads shortly before large LLM requests
sandbox_id:"<sandbox_id>" AND event_type:file.read AND timestamp:[<alert_time>-10m TO <alert_time>]
| fields timestamp, details.path, details.size
```

Look for bulk reads of source code, configuration files, or credential files before the large LLM request.

**Step 4 -- Response actions**

| Finding | Action |
|---------|--------|
| Payload contains encoded source code or secrets | Escalate to Critical. Revoke credentials. Review what data was sent. Engage legal if IP exfiltration confirmed. |
| Large payload is legitimate (long code file for refactoring) | Adjust LLM payload threshold for this team/project. Document as false positive. |
| Base64 in metadata fields (not prompt content) | Indicates deliberate encoding to evade content inspection. Escalate for review. |

---

### 6.3 DNS Tunneling Detection

**SIEM Rule**: aibox-002 (DNS Query Spike)
**Severity**: Medium | **SLA**: Next business day | **Channel**: Email
**MITRE ATT&CK**: T1071.004 (Application Layer Protocol: DNS)

**Alert trigger**: CoreDNS logs show >100 DNS queries/minute from a single sandbox, or Falco detects high-entropy subdomain patterns (`aibox_high_entropy_dns`).

**Step 1 -- Query DNS patterns**

```
# DNS queries from the sandbox in the alert window
sandbox_id:"<sandbox_id>" AND event_type:dns.query AND timestamp:[<start> TO <end>]
| stats count by details.query_name
| sort count desc
```

**Step 2 -- Entropy analysis**

DNS tunneling typically uses high-entropy subdomain labels (e.g., `aGVsbG8gd29ybGQ.evil.com`). Look for:
- Long subdomain labels (>30 characters)
- Subdomains that look like base64 or hex encoding
- Queries to domains not in the allowlist

```
# DNS queries to non-allowlisted domains
sandbox_id:"<sandbox_id>" AND event_type:dns.query
| where NOT details.query_name IN ("harbor.internal", "nexus.internal", "foundry.internal", "git.internal", "vault.internal")
| stats count by details.query_name
```

**Step 3 -- Correlate with network denials**

```
# Blocked network attempts from the same sandbox
sandbox_id:"<sandbox_id>" AND event_type:network.deny
| fields timestamp, details.destination, details.port
```

DNS tunneling often follows failed direct connection attempts -- the attacker tries direct connection first, gets blocked, then falls back to DNS.

**Step 4 -- Response actions**

| Finding | Action |
|---------|--------|
| High-entropy queries to unknown domains | Block the domain in CoreDNS. Stop the sandbox. Investigate what data was transmitted. |
| Spike from legitimate tool (npm install, Maven resolve) | Package manager activity causes DNS spikes. Tune threshold or add the package registry domain to the allowlist. |
| Queries to internal domains only | False positive. Normal development activity. No action needed. |

---

### 6.4 Credential Misuse After Sandbox Stop

**SIEM Rule**: aibox-008 (Credential Access After Sandbox Stop)
**Severity**: Critical | **SLA**: 15 minutes | **Channel**: PagerDuty
**MITRE ATT&CK**: T1528 (Steal Application Access Token)

**Alert trigger**: Vault audit shows credential use after the sandbox lifecycle log shows a `sandbox.stop` event for the same sandbox ID.

**Step 1 -- Confirm the timeline**

```
# Sandbox lifecycle events
sandbox_id:"<sandbox_id>" AND event_type:sandbox.*
| sort timestamp asc
| fields timestamp, event_type, user_id
```

```
# Credential events after the stop time
sandbox_id:"<sandbox_id>" AND event_type:credential.* AND timestamp:[<stop_time> TO now]
| fields timestamp, event_type, details.credential_type, details.target_system
```

**Step 2 -- Determine if credentials were exfiltrated**

```
# File access to credential-related paths before sandbox stop
sandbox_id:"<sandbox_id>" AND event_type:file.read AND timestamp:[<stop_time>-30m TO <stop_time>]
| where details.path LIKE "*credential*" OR details.path LIKE "*token*" OR details.path LIKE "*.env*"
```

```
# Network connections that could have exfiltrated credentials
sandbox_id:"<sandbox_id>" AND event_type:network.allow AND timestamp:[<stop_time>-30m TO <stop_time>]
| fields timestamp, details.destination, details.bytes
```

**Step 3 -- Response actions (immediate)**

1. **Revoke all credentials** issued to the sandbox: contact Vault admin or use `vault token revoke`
2. **Rotate affected secrets** in target systems the credentials accessed
3. **Check for unauthorized access** in the target systems using the stolen credentials
4. **Preserve audit trail**: `aibox audit verify` to ensure logs were not tampered

**Step 4 -- Root cause analysis**

| Finding | Likely Cause |
|---------|-------------|
| Credentials used from a different IP/host | Token was exfiltrated and used externally |
| Credentials used milliseconds after stop | Race condition in credential revocation; fix TTL/lifecycle binding |
| Credentials used from the same host, different sandbox | Credential leaked between sandboxes; review isolation boundaries |

---

### 6.5 Policy Violation Burst Investigation

**SIEM Rule**: aibox-007 (Policy Violation Burst)
**Severity**: High | **SLA**: 4 hours | **Channel**: Slack
**MITRE ATT&CK**: (no specific technique -- indicates automated evasion attempt)

**Alert trigger**: OPA decision log shows >15 policy denials in 5 minutes from the same sandbox, indicating automated attempts to circumvent policy controls.

**Step 1 -- Identify the denied actions**

```
# Policy denials from the sandbox
sandbox_id:"<sandbox_id>" AND event_type:policy.deny AND timestamp:[<start> TO <end>]
| stats count by details.rule_name, details.action
| sort count desc
```

**Step 2 -- Determine if actions are automated**

```
# Policy denials with timing analysis
sandbox_id:"<sandbox_id>" AND event_type:policy.deny
| sort timestamp asc
| fields timestamp, details.rule_name, details.action, details.input
```

Look for:
- Identical requests repeated at sub-second intervals (scripted attempts)
- Sequential variation in parameters (brute-force enumeration)
- Requests from an AI agent process vs. interactive terminal

**Step 3 -- Correlate with tool invocations**

```
# Tool invocations from the same sandbox
sandbox_id:"<sandbox_id>" AND event_type:tool.* AND timestamp:[<start>-5m TO <end>+5m]
| fields timestamp, event_type, details.command, details.risk_class, details.approver
```

**Step 4 -- Response actions**

| Finding | Action |
|---------|--------|
| AI agent repeatedly trying blocked operations | Review if the policy is too restrictive for the developer's workflow. Adjust team policy if legitimate. Alert the developer if the agent is misbehaving. |
| Manual scripted attempts to bypass policy | Escalate. This indicates intentional policy circumvention. Review session recording if available. |
| Burst from build/CI process | False positive. Build tools can trigger many policy evaluations. Tune threshold or add build-specific policy exceptions. |

---

### 6.6 General Investigation Tips

**Correlation across sources**: The most valuable investigations correlate events across multiple sources. Use the `sandbox_id` field to join events from Falco, Squid, OPA, Vault, and the LLM proxy into a single timeline.

**Timeline reconstruction**:

```
# Full timeline for a sandbox during an incident window
sandbox_id:"<sandbox_id>" AND timestamp:[<start> TO <end>]
| sort timestamp asc
| fields timestamp, event_type, source, severity, details
```

**Hash chain verification**: Before relying on logs for investigation, verify they have not been tampered with:

```bash
aibox audit verify
```

If the chain is broken, the `first_error` field tells you exactly where. Events before the break point are trustworthy; events after require independent verification.

**Session recording playback**: If session recording was enabled for the sandbox, replay the terminal session for full context:

```bash
aibox audit playback --session <session_id>
```

**Preserving evidence**: For incidents that may lead to disciplinary or legal action, ensure:
1. Hash chain verification passes (proves log integrity)
2. Storage-level immutability is confirmed (Object Lock status)
3. Session recordings are preserved with their encryption keys
4. A timeline report is generated before any remediation changes are made
